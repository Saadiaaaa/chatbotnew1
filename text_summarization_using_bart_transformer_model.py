# -*- coding: utf-8 -*-
"""Text_Summarization_using_BART_Transformer_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12KEzeZtPM47zvcZi_3SXpVX3AgY1MksV

Summarization with HuggingFace | NLP Projects:
"""

!pip install transformers torch accelerate bitsandbytes datasets

from google.colab import userdata
userdata.get('HF_TOKEN')

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from huggingface_hub import login
from google.colab import userdata
import datasets

Model_id = 'bitext/Bitext-customer-support-llm-chatbot-training-dataset'

# Log in to Hugging Face Hub using your token
# (Assuming you have stored your token in the userdata)
token = userdata.get('HF_TOKEN')
login(token=token)

bnb = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.bfloat16
)


# Load the dataset using the `datasets` library
# Removed use_auth_token as it is not supported by CsvConfig
dataset = datasets.load_dataset(Model_id)

from datasets import load_dataset

ds = load_dataset("bitext/Bitext-customer-support-llm-chatbot-training-dataset")

ds

ds['train']

ds['train'][1]['flags']

ds['train'][1]['instruction']

ds['train'][1]['category']

ds['train'][1]['intent']

ds['train'][1]['response']

"""#1. USING THE MODEL WITHOUT FINE TUNING
#LOADING THE BART MODEL

"""

from transformers import pipeline

text_summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

from transformers import pipeline

# Use a valid summarization model identifier
# Examples:
# - facebook/bart-large-cnn
# - google/pegasus-xsum
model_id = "facebook/bart-large-cnn"

summarizer = pipeline("summarization", model=model_id)

input_text = "This is a long text that I want to summarize. It can be a news article, a research paper, or any other long-form text. The goal is to generate a concise summary that captures the main points of the text."

# Adjust max_length based on input length
max_length = int(len(input_text) * 0.5)  # Adjust the factor as needed

query = input_text + "\nTL;DR:\n"
summary = summarizer(query, max_length=max_length, clean_up_tokenization_spaces=True)

print(summary[0]['summary_text'])

"""#2. FINE - TUNING MODEL


"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import TrainingArguments, Trainer

tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-base")

#tokenization

def preprocess_function(batch):
    source = batch['instruction']
    target = batch["response"]
    source_ids = tokenizer(source, truncation=True, padding="max_length", max_length=128)
    target_ids = tokenizer(target, truncation=True, padding="max_length", max_length=128)

    # Replace pad token id with -100 for labels to ignore padding in loss computation
    labels = target_ids["input_ids"]
    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in labels]

    return {
        "input_ids": source_ids["input_ids"],
        "attention_mask": source_ids["attention_mask"],
        "labels": labels
    }

ds_source = ds.map(preprocess_function, batched=True)

#use a p√Æpline as a high level helper
from transformers import pipeline
pipe = pipeline("summarization", model="facebook/bart-large-cnn")

# Import TrainingArguments from transformers
from transformers import TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="/content",  # Replace with your output directory
    per_device_train_batch_size=8,
    num_train_epochs=2,  # Adjust number of epochs as needed
    remove_unused_columns=False
)

# Create Trainer object
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=ds_source["train"],
    # Check if 'test' key exists before accessing it. If not use 'validation' or another available key
    eval_dataset=ds_source.get("test", ds_source.get("validation", None)) # Try 'test', fallback to 'validation' if 'test' is missing, fallback to None if both are missing
)

!pip install datasets
from datasets import DatasetDict

# ... (rest of your code) ...

# Get the original length of the training dataset
original_train_len = len(ds["train"])

# Define the split ratio
test_size = 0.2  # Adjust as needed

# Use stratified sampling to split the data
sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)

# **Replace 'your_target_column_name' with the actual name of your target column**
# For example, if your target column is 'category', use:
target_column = "category"  # Update with your column name

# Now use the correct target column name in the split function
for train_index, val_index in sss.split(ds["train"], ds["train"][target_column]):
    train_dataset = ds["train"].select(train_index)
    val_dataset = ds["train"].select(val_index)

# Preprocess the datasets
# Import DatasetDict from datasets to use it
ds_source = DatasetDict({"train": train_dataset.map(preprocess_function, batched=True),
                         "validation": val_dataset.map(preprocess_function, batched=True)})

!pip install scikit-learn
from sklearn.model_selection import train_test_split

# Reduce the test size to ensure it doesn't exceed the data size
test_size = min(0.15, len(ds["train"]) / (len(ds["train"]) + 1))  # Adjust as needed

# Now you can use train_test_split
train_indices, val_indices = train_test_split(range(original_train_len), test_size=test_size, random_state=42)

# Save the model and tokenizer after training
model.save_pretrained("/content/your_model_directory")
tokenizer.save_pretrained("/content/your_model_directory")

# upload the model
from huggingface_hub import notebook_login

notebook_login()